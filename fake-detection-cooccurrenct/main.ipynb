{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "import mlxtend\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import skimage\n",
    "from mlxtend.evaluate import confusion_matrix\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Dense, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint   \n",
    "from sklearn.metrics import classification_report  \n",
    "from skimage.feature import greycomatrix\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "np.set_printoptions(precision = 3, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width, img_height = 256, 256\n",
    "batch_size = 40\n",
    "epochs = 100                                 \n",
    "train_samples = 1200\n",
    "validation_samples = 400\n",
    "test_samples = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = './data/train/'\n",
    "validation_data_dir = './data/validate/'\n",
    "test_data_dir = './data/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def co_occurrence_horiz(image):\n",
    "    r_horiz = skimage.feature.graycomatrix(np.uint64(image[:,:,0]), [1], [0], levels=256, normed=True)\n",
    "    g_horiz = skimage.feature.graycomatrix(np.uint64(image[:,:,1]), [1], [0], levels=256, normed=True)\n",
    "    b_horiz = skimage.feature.graycomatrix(np.uint64(image[:,:,2]), [1], [0], levels=256, normed=True)\n",
    "    co_occurrence_horiz_img = np.dstack((r_horiz[:,:,0], g_horiz[:,:,0], b_horiz[:,:,0]))\n",
    "    return co_occurrence_horiz_img "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(preprocessing_function = co_occurrence_horiz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1199 images belonging to 2 classes.\n",
      "Found 400 images belonging to 2 classes.\n",
      "Found 400 images belonging to 2 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'gan': 0, 'real': 1}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator = datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        color_mode=\"rgb\",\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        class_mode='categorical')\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        color_mode=\"rgb\",\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        class_mode='categorical')\n",
    "test_generator = datagen.flow_from_directory(\n",
    "        test_data_dir,\n",
    "        color_mode=\"rgb\",\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size= 1,\n",
    "        shuffle=False,\n",
    "        class_mode='categorical')\n",
    "train_generator.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = train_generator.samples\n",
    "validation_samples = validation_generator.samples\n",
    "test_samples = test_generator.samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "\n",
      "systemMemory: 8.00 GB\n",
      "maxCacheSize: 2.67 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 20:42:56.361169: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-11-22 20:42:56.361577: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 254, 254, 32)      896       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 250, 250, 32)      25632     \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 125, 125, 32)     0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 123, 123, 64)      18496     \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 119, 119, 64)      102464    \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 59, 59, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 57, 57, 128)       73856     \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 53, 53, 128)       409728    \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 26, 26, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 86528)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               22151424  \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 514       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 22,848,802\n",
      "Trainable params: 22,848,802\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Convolution2D(32, (3, 3), input_shape=(img_width, img_height, 3), activation='relu'))\n",
    "model.add(Convolution2D(32, (5, 5)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2))) \n",
    "model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "model.add(Convolution2D(64, (5, 5)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
    "model.add(Convolution2D(128, (5, 5)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten()) \n",
    "model.add(Dense(256))\n",
    "model.add(Dense(256))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(learning_rate=0.000001), \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(filepath='./model/fake_image_detector.h5',\n",
    "                               verbose=1,\n",
    "                               save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 20:42:57.744868: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-11-22 20:42:58.078527: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/29 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.4579"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 20:43:43.779389: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 0.69312, saving model to ./model/fake_image_detector.h5\n",
      "29/29 [==============================] - 54s 2s/step - loss: 0.6932 - accuracy: 0.4579 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 2/100\n",
      "30/29 [==============================] - ETA: 0s - loss: 0.6931 - accuracy: 0.5004\n",
      "Epoch 2: val_loss improved from 0.69312 to 0.69310, saving model to ./model/fake_image_detector.h5\n",
      "29/29 [==============================] - 51s 2s/step - loss: 0.6931 - accuracy: 0.5004 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 3/100\n",
      "30/29 [==============================] - ETA: 0s - loss: 0.6931 - accuracy: 0.5004\n",
      "Epoch 3: val_loss improved from 0.69310 to 0.69307, saving model to ./model/fake_image_detector.h5\n",
      "29/29 [==============================] - 52s 2s/step - loss: 0.6931 - accuracy: 0.5004 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 4/100\n",
      "30/29 [==============================] - ETA: 0s - loss: 0.6931 - accuracy: 0.5046\n",
      "Epoch 4: val_loss improved from 0.69307 to 0.69304, saving model to ./model/fake_image_detector.h5\n",
      "29/29 [==============================] - 54s 2s/step - loss: 0.6931 - accuracy: 0.5046 - val_loss: 0.6930 - val_accuracy: 0.5775\n",
      "Epoch 5/100\n",
      "30/29 [==============================] - ETA: 0s - loss: 0.6930 - accuracy: 0.5004\n",
      "Epoch 5: val_loss improved from 0.69304 to 0.69300, saving model to ./model/fake_image_detector.h5\n",
      "29/29 [==============================] - 54s 2s/step - loss: 0.6930 - accuracy: 0.5004 - val_loss: 0.6930 - val_accuracy: 0.5000\n",
      "Epoch 6/100\n",
      "30/29 [==============================] - ETA: 0s - loss: 0.6930 - accuracy: 0.5388\n",
      "Epoch 6: val_loss improved from 0.69300 to 0.69295, saving model to ./model/fake_image_detector.h5\n",
      "29/29 [==============================] - 57s 2s/step - loss: 0.6930 - accuracy: 0.5388 - val_loss: 0.6929 - val_accuracy: 0.5000\n",
      "Epoch 7/100\n",
      "30/29 [==============================] - ETA: 0s - loss: 0.6929 - accuracy: 0.5004\n",
      "Epoch 7: val_loss improved from 0.69295 to 0.69288, saving model to ./model/fake_image_detector.h5\n",
      "29/29 [==============================] - 58s 2s/step - loss: 0.6929 - accuracy: 0.5004 - val_loss: 0.6929 - val_accuracy: 0.5000\n",
      "Epoch 8/100\n",
      "30/29 [==============================] - ETA: 0s - loss: 0.6929 - accuracy: 0.5004\n",
      "Epoch 8: val_loss improved from 0.69288 to 0.69282, saving model to ./model/fake_image_detector.h5\n",
      "29/29 [==============================] - 62s 2s/step - loss: 0.6929 - accuracy: 0.5004 - val_loss: 0.6928 - val_accuracy: 0.5000\n",
      "Epoch 9/100\n",
      "30/29 [==============================] - ETA: 0s - loss: 0.6928 - accuracy: 0.5405\n",
      "Epoch 9: val_loss improved from 0.69282 to 0.69273, saving model to ./model/fake_image_detector.h5\n",
      "29/29 [==============================] - 58s 2s/step - loss: 0.6928 - accuracy: 0.5405 - val_loss: 0.6927 - val_accuracy: 0.6800\n",
      "Epoch 10/100\n",
      "30/29 [==============================] - ETA: 0s - loss: 0.6927 - accuracy: 0.5004\n",
      "Epoch 10: val_loss improved from 0.69273 to 0.69261, saving model to ./model/fake_image_detector.h5\n",
      "29/29 [==============================] - 57s 2s/step - loss: 0.6927 - accuracy: 0.5004 - val_loss: 0.6926 - val_accuracy: 0.5000\n",
      "Epoch 11/100\n",
      "12/29 [===========>..................] - ETA: 30s - loss: 0.6927 - accuracy: 0.4154"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error: command buffer exited with error status.\n",
      "\tThe Metal Performance Shaders operations encoded on it may not have completed.\n",
      "\tError: \n",
      "\t(null)\n",
      "\tInternal Error (0000000e:Internal Error)\n",
      "\t<AGXG13GFamilyCommandBuffer: 0x29e1c8c00>\n",
      "    label = <none> \n",
      "    device = <AGXG13GDevice: 0x14b7c5800>\n",
      "        name = Apple M1 \n",
      "    commandQueue = <AGXG13GFamilyCommandQueue: 0x11b416c00>\n",
      "        label = <none> \n",
      "        device = <AGXG13GDevice: 0x14b7c5800>\n",
      "            name = Apple M1 \n",
      "    retainedReferences = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/29 [==============================] - ETA: 0s - loss: 0.6926 - accuracy: 0.5038\n",
      "Epoch 11: val_loss improved from 0.69261 to 0.69247, saving model to ./model/fake_image_detector.h5\n",
      "29/29 [==============================] - 59s 2s/step - loss: 0.6926 - accuracy: 0.5038 - val_loss: 0.6925 - val_accuracy: 0.5000\n",
      "Epoch 12/100\n",
      "30/29 [==============================] - ETA: 0s - loss: 0.6925 - accuracy: 0.6347\n",
      "Epoch 12: val_loss improved from 0.69247 to 0.69232, saving model to ./model/fake_image_detector.h5\n",
      "29/29 [==============================] - 59s 2s/step - loss: 0.6925 - accuracy: 0.6347 - val_loss: 0.6923 - val_accuracy: 0.5350\n",
      "Epoch 13/100\n",
      "30/29 [==============================] - ETA: 0s - loss: 0.6923 - accuracy: 0.5013\n",
      "Epoch 13: val_loss improved from 0.69232 to 0.69211, saving model to ./model/fake_image_detector.h5\n",
      "29/29 [==============================] - 58s 2s/step - loss: 0.6923 - accuracy: 0.5013 - val_loss: 0.6921 - val_accuracy: 0.5125\n",
      "Epoch 14/100\n",
      " 8/29 [=======>......................] - ETA: 35s - loss: 0.6918 - accuracy: 0.6313"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error: command buffer exited with error status.\n",
      "\tThe Metal Performance Shaders operations encoded on it may not have completed.\n",
      "\tError: \n",
      "\t(null)\n",
      "\tInternal Error (0000000e:Internal Error)\n",
      "\t<AGXG13GFamilyCommandBuffer: 0x29e1b35c0>\n",
      "    label = <none> \n",
      "    device = <AGXG13GDevice: 0x14b7c5800>\n",
      "        name = Apple M1 \n",
      "    commandQueue = <AGXG13GFamilyCommandQueue: 0x11b416c00>\n",
      "        label = <none> \n",
      "        device = <AGXG13GDevice: 0x14b7c5800>\n",
      "            name = Apple M1 \n",
      "    retainedReferences = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/29 [==============================] - ETA: 0s - loss: 0.6921 - accuracy: 0.5021\n",
      "Epoch 14: val_loss improved from 0.69211 to 0.69183, saving model to ./model/fake_image_detector.h5\n",
      "29/29 [==============================] - 59s 2s/step - loss: 0.6921 - accuracy: 0.5021 - val_loss: 0.6918 - val_accuracy: 0.5000\n",
      "Epoch 15/100\n",
      "30/29 [==============================] - ETA: 0s - loss: 0.6918 - accuracy: 0.5096\n",
      "Epoch 15: val_loss improved from 0.69183 to 0.69157, saving model to ./model/fake_image_detector.h5\n",
      "29/29 [==============================] - 60s 2s/step - loss: 0.6918 - accuracy: 0.5096 - val_loss: 0.6916 - val_accuracy: 0.5450\n",
      "Epoch 16/100\n",
      "30/29 [==============================] - ETA: 0s - loss: 0.6915 - accuracy: 0.6372\n",
      "Epoch 16: val_loss improved from 0.69157 to 0.69122, saving model to ./model/fake_image_detector.h5\n",
      "29/29 [==============================] - 61s 2s/step - loss: 0.6915 - accuracy: 0.6372 - val_loss: 0.6912 - val_accuracy: 0.5125\n",
      "Epoch 17/100\n",
      "30/29 [==============================] - ETA: 0s - loss: 0.6911 - accuracy: 0.5755\n",
      "Epoch 17: val_loss improved from 0.69122 to 0.69081, saving model to ./model/fake_image_detector.h5\n",
      "29/29 [==============================] - 58s 2s/step - loss: 0.6911 - accuracy: 0.5755 - val_loss: 0.6908 - val_accuracy: 0.6300\n",
      "Epoch 18/100\n",
      "30/29 [==============================] - ETA: 0s - loss: 0.6907 - accuracy: 0.5596\n",
      "Epoch 18: val_loss improved from 0.69081 to 0.69028, saving model to ./model/fake_image_detector.h5\n",
      "29/29 [==============================] - 58s 2s/step - loss: 0.6907 - accuracy: 0.5596 - val_loss: 0.6903 - val_accuracy: 0.5250\n",
      "Epoch 19/100\n",
      "30/29 [==============================] - ETA: 0s - loss: 0.6902 - accuracy: 0.6447\n",
      "Epoch 19: val_loss improved from 0.69028 to 0.68972, saving model to ./model/fake_image_detector.h5\n",
      "29/29 [==============================] - 61s 2s/step - loss: 0.6902 - accuracy: 0.6447 - val_loss: 0.6897 - val_accuracy: 0.6075\n",
      "Epoch 20/100\n",
      "30/29 [==============================] - ETA: 0s - loss: 0.6897 - accuracy: 0.6739\n",
      "Epoch 20: val_loss improved from 0.68972 to 0.68910, saving model to ./model/fake_image_detector.h5\n",
      "29/29 [==============================] - 63s 2s/step - loss: 0.6897 - accuracy: 0.6739 - val_loss: 0.6891 - val_accuracy: 0.8475\n",
      "Epoch 21/100\n",
      "30/29 [==============================] - ETA: 0s - loss: 0.6890 - accuracy: 0.6380\n",
      "Epoch 21: val_loss improved from 0.68910 to 0.68830, saving model to ./model/fake_image_detector.h5\n",
      "29/29 [==============================] - 70s 2s/step - loss: 0.6890 - accuracy: 0.6380 - val_loss: 0.6883 - val_accuracy: 0.6850\n",
      "Epoch 22/100\n",
      "30/29 [==============================] - ETA: 0s - loss: 0.6881 - accuracy: 0.6881\n",
      "Epoch 22: val_loss improved from 0.68830 to 0.68743, saving model to ./model/fake_image_detector.h5\n",
      "29/29 [==============================] - 77s 3s/step - loss: 0.6881 - accuracy: 0.6881 - val_loss: 0.6874 - val_accuracy: 0.7925\n",
      "Epoch 23/100\n",
      " 8/29 [=======>......................] - ETA: 43s - loss: 0.6874 - accuracy: 0.7743"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_generator,\n",
    "                              steps_per_epoch=train_samples / batch_size,\n",
    "                              epochs=epochs, callbacks=[checkpointer],\n",
    "                              validation_data=validation_generator,\n",
    "                              validation_steps=validation_samples / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0cef498f5d1fa7f5c3977465784b6d313c58b06a6ffc589e3aaf7a57d8c3a9db"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
